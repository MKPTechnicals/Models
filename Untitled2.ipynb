{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhjb9j5X1VLCIPkuxaEeX4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from scikeras.wrappers import KerasClassifier\n","import numpy as np\n","import pandas as pd\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","\n","# Import TPU-related modules\n","import tensorflow as tf\n","import os\n","\n","# Check if TPU is available\n","if 'COLAB_TPU_ADDR' not in os.environ:\n","    print(\"TPU not found. Running on CPU/GPU.\")\n","    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\" if tf.config.list_physical_devices(\"GPU\") else \"/cpu:0\")\n","else:\n","    print(\"Using TPU:\", os.environ['COLAB_TPU_ADDR'])\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","\n","with strategy.scope():\n","    # Load the diary entry data from the CSV file\n","    data = pd.read_csv('data.csv')\n","\n","    # Preprocess the diary entries\n","    text = data['Diary Entry'].str.lower().values\n","\n","    # Tokenize the text\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(text)\n","    total_words = len(tokenizer.word_index) + 1\n","\n","    # Create input sequences using n-grams\n","    input_sequences = []\n","    for line in text:\n","        token_list = tokenizer.texts_to_sequences([line])[0]\n","        for i in range(1, len(token_list)):\n","            n_gram_sequence = token_list[:i + 1]\n","            input_sequences.append(n_gram_sequence)\n","\n","    # Pad sequences for equal length\n","    max_sequence_length = max([len(seq) for seq in input_sequences])\n","    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n","\n","    # Split data into input and output\n","    X, y = input_sequences[:, :-1], input_sequences[:, -1]\n","    y = to_categorical(y, num_classes=total_words)\n","\n","    # Split the data into training and validation sets\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Define a ModelCheckpoint callback to save the best model\n","    best_model_path = 'best_model.h5'\n","    checkpoint = ModelCheckpoint(best_model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n","    # Define Early Stopping callback\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","    # Define TensorBoard callback\n","    tensorboard_callback = TensorBoard(log_dir='logs')\n","\n","    callbacks = [checkpoint, early_stopping, tensorboard_callback]\n","\n","    # Define a wrapper class with the embedding_dim, num_lstm_units, and batch_size parameters\n","    class MyKerasClassifier(KerasClassifier):\n","        def __init__(self, embedding_dim=50, num_lstm_units=50, **kwargs):\n","            self.embedding_dim = embedding_dim\n","            self.num_lstm_units = num_lstm_units\n","            super(MyKerasClassifier, self).__init__(**kwargs)\n","\n","        def _keras_build_fn(self, **kwargs):\n","            return create_lstm_model(embedding_dim=self.embedding_dim, num_lstm_units=self.num_lstm_units)\n","\n","        def fit(self, X, y, **kwargs):\n","            print(\"Training with hyperparameters:\")\n","            print(\"Embedding Dimension:\", self.embedding_dim)\n","            print(\"Number of LSTM Units:\", self.num_lstm_units)\n","            super().fit(X, y, **kwargs)\n","\n","    # Define a function to create the LSTM model\n","    def create_lstm_model(embedding_dim=50, num_lstm_units=50):\n","        model = Sequential()\n","        model.add(Embedding(total_words, embedding_dim, input_length=max_sequence_length - 1))\n","        model.add(LSTM(num_lstm_units))\n","        model.add(Dense(total_words, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam')\n","        return model\n","\n","    # Create a MyKerasClassifier wrapper for use in GridSearchCV\n","    model_wrapper = MyKerasClassifier(epochs=15, verbose=1)\n","\n","    # Define the hyperparameters to tune\n","    param_grid = {\n","        'embedding_dim': [350, 512, 700, 1024, 2048, 3500, 5120, 7000, 10240],\n","        'num_lstm_units': [350, 5120]\n","    }\n","\n","    # 350 : 350\n","    # 512 : 350\n","    # 700 : 350\n","    # 1024 : 350,5120\n","    # 2048 : 350,5120\n","    # 3500 : 350,5120\n","    # 5120 : 350,5120\n","    # 7000 : 350, 5120\n","    # 10240 : 350,5120\n","\n","    # Create GridSearchCV object\n","    grid = GridSearchCV(estimator=model_wrapper, param_grid=param_grid, scoring='neg_log_loss', cv=5, error_score='raise')\n","    grid_result = grid.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks)\n","\n","    # Display the best hyperparameters\n","    print(\"Best Hyperparameters: \", grid_result.best_params_)\n"],"metadata":{"id":"j8-ogZYLcw9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.optimizers import Adam  # Import Adam optimizer\n","\n","# Load the diary entry data from the CSV file\n","data = pd.read_csv('data.csv')\n","\n","# Preprocess the diary entries\n","text = data['Diary Entry'].str.lower().values\n","dates = data['Date']\n","\n","# Tokenize the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(text)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Create input sequences using n-grams\n","input_sequences = []\n","for line in text:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Pad sequences for equal length\n","max_sequence_length = max([len(seq) for seq in input_sequences])\n","input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n","\n","# Split data into input and output\n","X, y = input_sequences[:, :-1], input_sequences[:, -1]\n","y = to_categorical(y, num_classes=total_words)\n","\n","# Split the data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define callbacks\n","best_model_path = 'best_model.h5'\n","checkpoint = ModelCheckpoint(best_model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","callbacks = [checkpoint, early_stopping]\n","\n","# Define learning rate\n","learning_rate = 0.001\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(total_words, 850, input_length=max_sequence_length-1))\n","model.add(LSTM(1024))\n","model.add(Dense(total_words, activation='softmax'))\n","\n","# Compile the model with the specified learning rate\n","optimizer = Adam(learning_rate=learning_rate)\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","\n","# Train the model with callbacks\n","history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, verbose=1, callbacks=callbacks)\n","\n","# Load the best model\n","best_model = load_model(best_model_path)\n","\n","# Monitor training history for accuracy\n","print(\"Validation Loss:\", history.history['val_loss'])\n","\n","# Function to generate a diary entry prediction\n","def predict_diary_entry(date, model, tokenizer, max_sequence_length, temperature=0.5, stop_word=None):\n","    seed_text = date\n","    predicted_text = seed_text\n","\n","    for _ in range(max_sequence_length):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n","        predicted = model.predict(token_list, verbose=0)[0]\n","\n","        # Apply temperature to control randomness\n","        predicted = np.log(predicted) / temperature\n","        exp_predicted = np.exp(predicted)\n","        predicted = exp_predicted / np.sum(exp_predicted)\n","\n","        # Sample the word index based on the predicted probabilities\n","        predicted_word_index = np.random.choice(len(predicted), p=predicted)\n","\n","        # Convert the word index to the actual word\n","        predicted_word = tokenizer.index_word.get(predicted_word_index, \"\")\n","\n","        if predicted_word == stop_word:\n","            break\n","\n","        seed_text += \" \" + predicted_word\n","        predicted_text += \" \" + predicted_word\n","\n","    return predicted_text\n","\n","# Example usage:\n","input_date = \"2024-02-14\"\n","predicted_diary_entry = predict_diary_entry(input_date, best_model, tokenizer, max_sequence_length, temperature=0.7, stop_word='[EOS]')\n","print(predicted_diary_entry)\n","\n","print(model.summary())  # Print model summary\n"],"metadata":{"id":"FdOwNuiFcJnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPn6BQfFb7Fw"},"outputs":[],"source":["!pip install gpt_2_simple\n","\n","import gpt_2_simple as gpt2\n","import os\n","import pandas as pd\n","import tensorflow as tf\n","\n","# Load your dataset with Date and Diary Entry columns\n","# Assuming it's a CSV file named 'data2.csv'\n","diary_data = pd.read_csv('data2.csv')\n","\n","# Preprocess data\n","# Assuming 'Date' and 'Diary Entry' are the column names\n","data = (diary_data['Date'] + '\\n' + diary_data['Diary Entry']).tolist()\n","\n","# Save preprocessed data to a text file\n","with open('diary.txt', 'w') as file:\n","    for entry in data:\n","        file.write(entry + '\\n')\n","\n","model_name = \"124M\"\n","if not os.path.isdir(os.path.join(\"models\", model_name)):\n","    print(f\"Downloading {model_name} model...\")\n","    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n","\n","# Reset TensorFlow graph\n","#gpt2.reset_session(sess)\n","\n","# Start TensorFlow session\n","sess = gpt2.start_tf_sess()\n","\n","# Fine-tune the GPT-2 model with reduced batch size and gradient accumulation\n","file_name = 'diary.txt'  # Provide the correct file name here\n","gpt2.finetune(sess,\n","              file_name,\n","              model_name=model_name,\n","              steps=900,   # steps is max number of training steps\n","              batch_size=2,  # Reduce batch size\n","              accumulate_gradients=2)  # Accumulate gradients every 2 steps\n","\n","# Generate text using the fine-tuned model\n","generated_text = gpt2.generate(sess)\n","print(generated_text)\n"]}]}